{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import os\n",
    "import pandas as pd"
   ],
   "execution_count":1,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"aFD4XSBHTZIqzTXZzKnZ1r",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def find_csv(path='\/data\/notebook_files\/'):\n",
    "    df_name = ''\n",
    "    for _, _, arquivos in os.walk(path):\n",
    "        for arquivo in arquivos:\n",
    "            if arquivo.endswith('.csv'):\n",
    "                df_name = arquivo\n",
    "    return df_name\n",
    "\n",
    "def open_dataframe(**params):\n",
    "    if params is None:\n",
    "        df = pd.read_csv(find_csv(), thousands=',', decimal='.', engine='c')\n",
    "    else:\n",
    "        df = pd.read_csv(find_csv(), **params)\n",
    "    return df"
   ],
   "execution_count":2,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"3fuN0NJmG334OK2UYR4tkD",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class ETLBasic:\n",
    "    def __init__(self, df_name, df):\n",
    "        self.df_name = df_name\n",
    "        self.df = df\n",
    "        self.columns = self.df.columns.values.tolist()\n",
    "        self.statistics_numerics_df = {}\n",
    "        self.statistics_categoricals_df = {}\n",
    "    \n",
    "    def df_header(self, head=10):\n",
    "        return self.df.head(head)\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def format_date(self, format_date='%d\/%m\/%Y', column=None):\n",
    "        self.df[column] = pd.to_datetime(self.df[column])\n",
    "        self.df[column] = self.df[column].dt.strftime(format_date)\n",
    "        return self.get_df()\n",
    "    \n",
    "    def convert_to_numeric(self, columns=None):\n",
    "        if columns is None:\n",
    "            print('Colunas não fornecidas: \\n\\t\\t -> Colunas: ', columns)\n",
    "        else:\n",
    "            try:\n",
    "                for column in columns:\n",
    "                    self.df[column] = self.df[column].str.replace(r'[,]', '', regex=True)\n",
    "                    self.df[column] = pd.to_numeric(self.df[column],errors='coerce')\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "            else:\n",
    "                return self.get_df()\n",
    "\n",
    "    def date_as_index(self, column):\n",
    "        self.df.set_index(column, inplace=True)\n",
    "        return self.get_df()\n",
    "    \n",
    "    def df_between_time(self, start_time, end_time):\n",
    "        return self.df.between_time(start_time=start_time, end_time=end_time)\n",
    "    \n",
    "    def describe_df(self, include=None, exclude=None, percentiles=None):\n",
    "        return self.df.describe(include=include, exclude=exclude, percentiles=percentiles)\n",
    "\n",
    "    def info_df(self):\n",
    "        return self.df.info()\n",
    "    \n",
    "    def show_informations_df(self):\n",
    "        print(self.info_df())\n",
    "        self.describe_df()\n",
    "    \n",
    "    def get_statistics_numerics_df(self):\n",
    "        columns = self.describe_df(include=float).columns.to_list()\n",
    "        for column in columns:\n",
    "            self.statistics_numerics_df[column] = {\n",
    "                    'count': self.df[column].count(),\n",
    "                    'mean': self.df[column].mean(),\n",
    "                    'median': self.df[column].median(),\n",
    "                    'variance': self.df[column].var(),\n",
    "                    'std': self.df[column].std(),\n",
    "                    'mode': self.df[column].mode().values[0],\n",
    "                    'max': self.df[column].max(),\n",
    "                    'min': self.df[column].min(),\n",
    "                    'duplicates': self.df[column].duplicated(keep=False).sum(),\n",
    "                    'NA_values': self.df[column].isnull().sum(),\n",
    "                    'Have_NA_values': self.df[column].isnull().any(),\n",
    "                    '%_NA_values': f'{round((self.df[column].isnull().sum() \/ self.df.shape[0]), 2) * 100}%',\n",
    "                    'zeros_count': (self.df[column] == 0).sum()\n",
    "            }\n",
    "\n",
    "        df_statistics_numerics = pd.DataFrame(self.statistics_numerics_df)\n",
    "\n",
    "        return df_statistics_numerics\n",
    "    \n",
    "    def get_statistics_categoricals_df(self):\n",
    "        columns = self.describe_df(include=object).columns.to_list()\n",
    "\n",
    "        for column in columns:\n",
    "            self.statistics_categoricals_df[column] = {\n",
    "                'mode': self.df[column].mode.values[0],\n",
    "                'count': self.df[column].count(),\n",
    "                'NA_values': self.df[column].isnull().sum(),\n",
    "                'Have_NA_values': self.df[column].isnull().any(),\n",
    "                '%_NA_values': f'{round((self.df[column].isnull().sum() \/ self.df.shape[0]), 2) * 100}%',\n",
    "                'duplicates': self.df[column].duplicated(keep = False).count()\n",
    "            }\n",
    "\n",
    "        df_statistics_categoricals = pd.DataFrame(self.statistics_numerics_df)\n",
    "\n",
    "        return df_statistics_categoricals\n",
    "\n",
    "    def sort_values_duplicated(self, column):\n",
    "        df_duplicates = self.df[column][self.df[column].duplicated() == True].sort_values()\n",
    "        return df_duplicates\n",
    "    \n",
    "    def drop_duplicates(self, subset=None):\n",
    "        return self.df.drop_duplicates(subset=subset, keep=False, inplace=True)\n",
    "    \n",
    "    def dropna_df(self, axis=0, subset=None, thresh=None, how='any'):\n",
    "        return self.df.dropna(axis=axis, inplace=True, subset=subset, thresh=thresh, how=how)\n",
    "    \n",
    "    def fillna(self, value, axis=0):\n",
    "        return self.df.fillna(value=value, axis=axis, inplace=True)\n",
    "    \n",
    "    def eval_df(self, exp):\n",
    "        return self.df.eval(exp, inplace=True)\n",
    "    \n",
    "    def df_value_counts(self, columns=None, subset=None, normalize=False, sort=True, ascending=False, dropna=True):\n",
    "        if columns is None:\n",
    "            return self.df.value_counts(subset=subset, normalize=normalize, sort=sort, ascending=ascending, dropna=dropna)\n",
    "        else:\n",
    "            return self.df[columns].value_counts(subset=subset, normalize=normalize, sort=sort, ascending=ascending, dropna=dropna)"
   ],
   "execution_count":3,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"VGV8LqzSZ7Cqx0XCkwjLuV",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class Model:    \n",
    "    def __init__(self, model, data, column_target):\n",
    "        self.model = model\n",
    "        self.x = data.drop(column_target, axis=1)\n",
    "        self.y = data[column_target]\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = None, None, None, None\n",
    "        self.y_train_pred, self.y_test_pred = None, None\n",
    "        self.train_accuracy, self.test_accuracy = None, None\n",
    "    \n",
    "    def scale_x(self):\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        x_new = pd.DataFrame(scaler.fit_transform(self.x), index=self.x.index, columns=self.x.columns)\n",
    "        return x_new\n",
    "    \n",
    "    def find_best_config_model(self, task='classification', estimator_list=['lgbm']):\n",
    "        from flaml import AutoML\n",
    "\n",
    "        automl = AutoML()\n",
    "        automl.fit(x_train=self.x_train, y_train=self.y_train, task=task, estimator_list=estimator_list)\n",
    "        best_params = automl.best_config\n",
    "        self.model = self.model(**best_params)\n",
    "        return self.model, best_params\n",
    "    \n",
    "    def train_test_split_data(self, test_size=0.3, train_size=0.7, random_state=None, shuffle=True, stratify=None):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.scale_x(), self.y, test_size=test_size, train_size=train_size, random_state=random_state, shuffle=shuffle, stratify=stratify)\n",
    "        return self.x_train, self.x_test, self.y_train, self.y_test\n",
    "\n",
    "    def fit_model(self, eval_metric='l1', early_stopping_rounds=1000):\n",
    "        self.model.fit(self.x_train, self.y_train, eval_set=[(self.x_test, self.y_test)], eval_metric=eval_metric, early_stopping_rounds=early_stopping_rounds)\n",
    "    \n",
    "    def predict(self):\n",
    "        self.y_train_pred = self.model.predict(self.x_train)\n",
    "        self.y_test_pred = self.model.predict(self.x_test)\n",
    "        return self.y_train_pred, self.y_test_pred\n",
    "\n",
    "    def accuracy_model(self):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "\n",
    "        self.train_accuracy = accuracy_score(self.y_train, self.y_train_pred)\n",
    "        self.test_accuracy = accuracy_score(self.y_test, self.y_test_pred)\n",
    "        print(f'Acurácia no conjunto de treinamento: {self.train_accuracy:.4f}')\n",
    "        print(f'Acurácia no conjunto de validação: {self.test_accuracy:.4f}')\n",
    "        return self.train_accuracy, self.test_accuracy\n",
    "    \n",
    "    def show_confusion_matrix(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "        \n",
    "        confusion_matrix = confusion_matrix(self.y_test, self.y_test_pred, labels=self.model.classes_)\n",
    "        display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=self.model.classes_)\n",
    "        display.plot()\n",
    "        plt.show()\n",
    "    \n",
    "    def exist_overfitting(self, cv=5):\n",
    "        import numpy as np\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "\n",
    "        cv_scores = cross_val_score(self.model, self.x_train, self.y_train, cv=cv)\n",
    "        mean_cv_score = np.mean(cv_scores)\n",
    "        print(f'Acurácia média na validação cruzada: {mean_cv_score:.4f}')\n",
    "        if self.test_accuracy >= mean_cv_score:\n",
    "            print('Não houve evidências de overfitting.')\n",
    "        else:\n",
    "            print('Houve evidências de overfitting.')\n",
    "    \n",
    "    def execute_model(self, eval_metric, early_stopping_rounds):\n",
    "        self.fit_model(eval_metric, early_stopping_rounds)\n",
    "        self.predict()\n",
    "        self.accuracy_model()\n",
    "        self.show_confusion_matrix()\n",
    "        self.exist_overfitting()\n"
   ],
   "execution_count":4,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"GzIljEBPfWDoVx0S5lKfpT",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "df_name = find_csv()\n",
    "df = open_dataframe()\n",
    "\n",
    "etl = ETLBasic(df_name=df_name, df=df)\n",
    "etl.df_header()"
   ],
   "execution_count":7,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"tTRpu4jKIKtNy7OuUdAJOi",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"REACTIVE",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    {
     "name":"lightgbm",
     "version":"3.3.5",
     "source":"PIP"
    }
   ],
   "report_row_ids":[
    
   ],
   "version":2
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}